{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pressing-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-embassy",
   "metadata": {},
   "source": [
    "### Data Load: Load banglore home prices into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-tonight",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Bengaluru_House_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a14733f23027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/Bengaluru_House_Data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m         )\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\KerasPython37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Bengaluru_House_Data.csv'"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/Bengaluru_House_Data.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('area_type')['area_type'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['area_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['area_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-exhibit",
   "metadata": {},
   "source": [
    "#### Drop features that are not required to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop(['area_type','society','balcony','availability'],axis='columns')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-stevens",
   "metadata": {},
   "source": [
    "### Data Cleaning: Handle NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.dropna()\n",
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-wallet",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Add new feature(integer) for bhk (Bedrooms Hall Kitchen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['bhk'] = df3['size'].apply(lambda x : int(x.split(' ')[0]))\n",
    "df3.bhk.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-rendering",
   "metadata": {},
   "source": [
    "#### Explore total_sqft feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.bhk >20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.total_sqft.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[~df3['total_sqft'].apply(is_float)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sqrt_to_num(x):\n",
    "    tokens = x.split('-')\n",
    "    if len(tokens) == 2:\n",
    "        return (float(tokens[0])+float(tokens[1]))/2\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sqrt_to_num('2166')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sqrt_to_num('2100 - 2850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sqrt_to_num('34.46Sq. Meter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()\n",
    "df4['total_sqft'] = df4['total_sqft'].apply(convert_sqrt_to_num)\n",
    "df4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2100+2850)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-disco",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Add new feature called price per square feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()\n",
    "df5['price_per_sqft'] = df5['price']*100000/df5['total_sqft']\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df5.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.location = df5.location.apply(lambda x : x.strip())\n",
    "location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending=False)\n",
    "location_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(location_stats[location_stats<=10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_stats_less_than_10 = location_stats[location_stats<=10]\n",
    "location_stats_less_than_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df5.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.location = df5.location.apply(lambda x : 'other' if x in location_stats_less_than_10 else x)\n",
    "len(df5.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-queensland",
   "metadata": {},
   "source": [
    "### Outlier Removal Using Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[df5.total_sqft/df5.bhk<300].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5[~(df5.total_sqft/df5.bhk<300)]\n",
    "df6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-hospital",
   "metadata": {},
   "source": [
    "### Outlier Removal Using Standard Deviation and Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.price_per_sqft.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pps_outliers(df):\n",
    "    df_out = pd.DataFrame()\n",
    "    for key, subdf in df.groupby('location'):\n",
    "        m = np.mean(subdf.price_per_sqft)\n",
    "        st = np.std(subdf.price_per_sqft)\n",
    "        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n",
    "        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n",
    "    return df_out\n",
    "df7 = remove_pps_outliers(df6)\n",
    "df7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-section",
   "metadata": {},
   "source": [
    "#### Let's check if for a given location how does the 2 BHK and 3 BHK property prices look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_chart(df,location):\n",
    "    bhk2 = df[(df.location==location) & (df.bhk==2)]\n",
    "    bhk3 = df[(df.location==location) & (df.bhk==3)]\n",
    "    matplotlib.rcParams['figure.figsize'] = (15,10)\n",
    "    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK', s=50)\n",
    "    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='green',label='3 BHK', s=50)\n",
    "    plt.xlabel(\"Total Square Feet Area\")\n",
    "    plt.ylabel(\"Price (Lakh Indian Rupees)\")\n",
    "    plt.title(location)\n",
    "    plt.legend()\n",
    "    \n",
    "plot_scatter_chart(df7,\"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(df7,\"Hebbal\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "streaming-craft",
   "metadata": {},
   "source": [
    "We should also remove properties where for same location, the price of (for example) 3 bedroom apartment is less than 2 bedroom apartment (with same square ft area). What we will do is for a given location, we will build a dictionary of stats per bhk, i.e.\n",
    "\n",
    "{\n",
    "    '1' : {\n",
    "        'mean': 4000,\n",
    "        'std: 2000,\n",
    "        'count': 34\n",
    "    },\n",
    "    '2' : {\n",
    "        'mean': 4300,\n",
    "        'std: 2300,\n",
    "        'count': 22\n",
    "    },    \n",
    "}\n",
    "\n",
    "Now we can remove those 2 BHK apartments whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bhk_outliers(df):\n",
    "    exclude_indices = np.array([])\n",
    "    for location, location_df in df.groupby('location'):\n",
    "        bhk_stats = {}\n",
    "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
    "            bhk_stats[bhk] = {\n",
    "                'mean': np.mean(bhk_df.price_per_sqft),\n",
    "                'std': np.std(bhk_df.price_per_sqft),\n",
    "                'count': bhk_df.shape[0]\n",
    "            }\n",
    "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
    "            stats = bhk_stats.get(bhk-1)\n",
    "            if stats and stats['count']>5:\n",
    "                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n",
    "    return df.drop(exclude_indices,axis='index')\n",
    "df8 = remove_bhk_outliers(df7)\n",
    "# df8 = df7.copy()\n",
    "df8.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-sapphire",
   "metadata": {},
   "source": [
    "#### Plot same scatter chart again to visualize price_per_sqft for 2 BHK and 3 BHK properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(df8,\"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(df8,\"Hebbal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.hist(df8.price_per_sqft,rwidth=0.8)\n",
    "plt.xlabel(\"Price Per Square Feet\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-egyptian",
   "metadata": {},
   "source": [
    "#### Outlier Removal Using Bathrooms Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.bath.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8[df8.bath>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df8.bath,rwidth=0.8)\n",
    "plt.xlabel(\"Number of bathrooms\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8[df8.bath>df8.bhk+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-advertiser",
   "metadata": {},
   "source": [
    "#### Again the business manager has a conversation with you (i.e. a data scientist) that if you have 4 bedroom home and even if you have bathroom in all 4 rooms plus one guest bathroom, you will have total bath = total bed + 1 max. Anything above that is an outlier or a data error and can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df8[df8.bath<df8.bhk+2]\n",
    "df9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df9.drop(['size','price_per_sqft'],axis='columns')\n",
    "df10.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-automation",
   "metadata": {},
   "source": [
    "### Use One Hot Encoding For Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df10.location)\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.concat([df10,dummies.drop('other',axis = 'columns')],axis = 'columns')\n",
    "df11.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = df11.drop('location',axis='columns')\n",
    "df12.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-conservation",
   "metadata": {},
   "source": [
    "### Build a Model Now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df12.drop('price',axis = 'columns')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df12.price\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_clf = LinearRegression()\n",
    "lr_clf.fit(X_train,y_train)\n",
    "lr_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-bulgaria",
   "metadata": {},
   "source": [
    "### Use K Fold cross validation to measure accuracy of our LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "cross_val_score(LinearRegression(), X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-edition",
   "metadata": {},
   "source": [
    "#### We can see that in 5 iterations we get a score above 80% all the time. This is pretty good but we want to test few other algorithms for regression to see if we can get even better score. We will use GridSearchCV for this purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-nation",
   "metadata": {},
   "source": [
    "### Find best model using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def find_best_model_using_gridsearchcv(X,y):\n",
    "    algos = {\n",
    "        'linear_regression' : {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                'normalize': [True, False]\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {\n",
    "                'alpha': [1,2],\n",
    "                'selection': ['random', 'cyclic']\n",
    "            }\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeRegressor(),\n",
    "            'params': {\n",
    "                'criterion' : ['mse','friedman_mse'],\n",
    "                'splitter': ['best','random']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits = 5,test_size = 0.2,random_state = 0)\n",
    "    for algo_name,config in algos.items():\n",
    "        gs = GridSearchCV(config['model'],config['params'],cv = cv,return_train_score=False)\n",
    "        gs.fit(X,y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(scores,columns = ['model','best_score','best_params'])\n",
    " \n",
    "find_best_model_using_gridsearchcv(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-breath",
   "metadata": {},
   "source": [
    "#### Based on above results we can say that LinearRegression gives the best score. Hence we will use that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-celebrity",
   "metadata": {},
   "source": [
    "### Test the model for few properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(location,sqft,bath,bhk):    \n",
    "    loc_index = np.where(X.columns==location)[0][0]\n",
    "\n",
    "    x = np.zeros(len(X.columns))\n",
    "    x[0] = sqft\n",
    "    x[1] = bath\n",
    "    x[2] = bhk\n",
    "    if loc_index >= 0:\n",
    "        x[loc_index] = 1\n",
    "\n",
    "    return lr_clf.predict([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('1st Phase JP Nagar',1000, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('1st Phase JP Nagar',1000, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('Indira Nagar',1000, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('Indira Nagar',1000, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-expense",
   "metadata": {},
   "source": [
    "### Export the tested model to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('banglore_home_prices_model.pickle','wb') as f:\n",
    "    pickle.dump(lr_clf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-method",
   "metadata": {},
   "source": [
    "### Export location and column information to a file that will be useful later on in our prediction application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "    'data_columns' : [col.lower() for col in X.columns]\n",
    "}\n",
    "with open(\"columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-fellow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
